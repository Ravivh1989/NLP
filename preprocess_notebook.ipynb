{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contractions Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_dict = {     \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I had\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"iit will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they had\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expanding Contractions\n",
    "Contraction helps to expand the word form like \"ain't\": \"am not\". Contractions file has been created in my github which we will be importing to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am not going there. you will have to go alone.\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "def expand_contractions(text, contractions_dict):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contractions_dict.keys())),\n",
    "                                      flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contractions_dict.get(match) \\\n",
    "            if contractions_dict.get(match) \\\n",
    "            else contractions_dict.get(match.lower())\n",
    "        expanded_contraction = expanded_contraction\n",
    "        return expanded_contraction\n",
    "\n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "def main():\n",
    "    text = \"\"\"I ain't going there. You'll have to go alone.\"\"\"\n",
    "    \n",
    "    text=expand_contractions(text,contractions_dict)\n",
    "    print (text)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing\n",
    "lemmatize the text so as to get its root form eg: functions,funtionality as function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the function of this fan is awesome\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stopword = stopwords.words('english')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(text):\n",
    "    \"\"\"\n",
    "    take string input and lemmatize the words.\n",
    "    use WordNetLemmatizer to lemmatize the words.\n",
    "    \"\"\"\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    lemmatized_word = [wordnet_lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "    return (\" \".join(lemmatized_word))\n",
    "\n",
    "def main():\n",
    "    text = \"the functions of this fan is awesome\"\n",
    "    print (lemmatize(text))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove numbers\n",
    "Removing numbers from the text like \"1,2,3,4,5â€¦\" We usually remove numbers when we do text clustering or getting keyphrases as we numbers doesn't give much importance to get the main words. To remove numbers, you can use: .isnumeric() else .isdigit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was  people standing right next to me at pm.\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def remove_numbers(text):\n",
    "    \"\"\"\n",
    "    take string input and return a clean text without numbers. \n",
    "    Use regex to discard the numbers.\n",
    "    \"\"\"\n",
    "    output = ''.join(c for c in text if not c.isdigit())\n",
    "    return output\n",
    "    \n",
    "def main():\n",
    "    text = \"There was 200 people standing right next to me at 2pm.\"\n",
    "    print (remove_numbers(text))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Punctuations\n",
    "Removing punctuation from the text like \".?!\" and also the symbols like \"@#$\" .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello how are you doing\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "def remove_punct(text):\n",
    "    \"\"\"\n",
    "    take string input and clean string without punctuations.\n",
    "    use regex to remove the punctuations.\n",
    "    \"\"\"\n",
    "    return ''.join(c for c in text if c not in punctuation)\n",
    "    \n",
    "def main():\n",
    "    text = \"Hello! how are you doing?\"\n",
    "    print (remove_punct(text))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words removal\n",
    "Remove irrelevant words using nltk stop words like \"is,the,a\" etc from the sentences as they don't carry any information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter miserable , lonely boy imagine . He â€™ shunned relatives , Dursley â€™ , raised since infant . He â€™ forced live cupboard stairs , forced wear cousin Dudley â€™ hand-me-down clothes , forced go neighbour â€™ house rest family something fun . Yes , â€™ miserable get .\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    \"\"\"\n",
    "    removes all the stop words like \"is,the,a, etc.\"\n",
    "    5 lines of code can be written in one line as:\n",
    "        return ' '.join([w for w in word_tokenize(sentence) if not w in stop_words]) \n",
    "    \"\"\"\n",
    "    clean_sent =[]\n",
    "    for w in word_tokenize(sentence):\n",
    "        if not w in stop_words:\n",
    "            clean_sent.append(w)\n",
    "    return \" \".join(clean_sent)\n",
    "\n",
    "def main():\n",
    "    text = \"\"\"Harry Potter is the most miserable, lonely boy you can imagine. Heâ€™s shunned by his relatives, the Dursleyâ€™s, that have raised him since he was an infant. Heâ€™s forced to live in the cupboard under the stairs, forced to wear his cousin Dudleyâ€™s hand-me-down clothes, and forced to go to his neighbourâ€™s house when the rest of the family is doing something fun. Yes, heâ€™s just about as miserable as you can get.\"\"\"\n",
    "    cleaned_text = remove_stopwords(text)\n",
    "    print (cleaned_text)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Tags\n",
    "Removing html tags from the text like \"\" using regex.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world!\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import nltk\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def remove_Tags(text):\n",
    "    \"\"\"\n",
    "    take string input and clean string without tags.\n",
    "    use regex to remove the html tags.\n",
    "    \"\"\"\n",
    "    cleaned_text = re.sub('<[^<]+?>','', text)\n",
    "    return cleaned_text\n",
    "    \n",
    "def main():\n",
    "    text = \"\"\"<head><body>hello world!</body></head>\"\"\"\n",
    "    print (remove_Tags(text))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence tokenize\n",
    "Tokenize sentences if the there are more than 1 sentence i.e breaking the sentences to list of sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Harry Potter is the most miserable, lonely boy you can imagine.', \"He's shunned by his relatives, the Dursley's, that have raised him since he was an infant.\", \"He's forced to live in the cupboard under the stairs, forced to wear his cousin Dudley's hand-me-down clothes, and forced to go to his neighbour's house when the rest of the family is doing something fun.\", \"Yes, he's just about as miserable as you can get.\"]\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def sentence_tokenize(text):\n",
    "    \"\"\"\n",
    "    take string input and return list of sentences.\n",
    "    use nltk.sent_tokenize() to split the sentences.\n",
    "    \"\"\"\n",
    "    sent_list=[]\n",
    "    for w in nltk.sent_tokenize(text):\n",
    "        sent_list.append(w)\n",
    "    return sent_list\n",
    "    \n",
    "def main():\n",
    "    text = \"\"\"Harry Potter is the most miserable, lonely boy you can imagine. He's shunned by his relatives, the Dursley's, that have raised him since he was an infant. He's forced to live in the cupboard under the stairs, forced to wear his cousin Dudley's hand-me-down clothes, and forced to go to his neighbour's house when the rest of the family is doing something fun. Yes, he's just about as miserable as you can get.\"\"\"\n",
    "    print (sentence_tokenize(text))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spell Check\n",
    "correct the incorrect spelled words like \"wrld\" to \"world\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a world of hope\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import nltk\n",
    "from autocorrect import spell\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def autospell(text):\n",
    "    \"\"\"\n",
    "    correct the spelling of the word.\n",
    "    \"\"\"\n",
    "    spells = [spell(w) for w in (nltk.word_tokenize(text))]\n",
    "    return \" \".join(spells)\n",
    "\n",
    "\n",
    "def main():\n",
    "    text = \"This is a wrld of hope\"\n",
    "    spell_text = autospell(text)\n",
    "    print (spell_text)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the function of this fan is awesom\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stopword = stopwords.words('english')\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "\n",
    "\n",
    "def stemming(text):\n",
    "    \"\"\"\n",
    "    take string input and stem the words.\n",
    "    use snowball_stemmer to stem the string.\n",
    "    \"\"\"\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    stemmed_word = [snowball_stemmer.stem(word) for word in word_tokens]\n",
    "    return \" \".join(stemmed_word)\n",
    "    \n",
    "def main():\n",
    "    text = \"the functions of this fan is awesome\"\n",
    "    print (stemming(text))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To lower\n",
    "Converting text to lower case as in, converting \"Hello\" to \"hello\" or \"HELLO\" to \"hello\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harry potter is the most miserable , lonely boy you can imagine . he â€™ s shunned by his relatives , the dursley â€™ s , that have raised him since he was an infant . he â€™ s forced to live in the cupboard under the stairs , forced to wear his cousin dudley â€™ s hand-me-down clothes , and forced to go to his neighbour â€™ s house when the rest of the family is doing something fun . yes , he â€™ s just about as miserable as you can get .\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def to_lower(text):\n",
    "    \"\"\"\n",
    "    Converting text to lower case as in, converting \"Hello\" to \"hello\" or \"HELLO\" to \"hello\".\n",
    "    \"\"\"\n",
    "    return ' '.join([w.lower() for w in word_tokenize(text)])\n",
    "\n",
    "\n",
    "def main():\n",
    "    text = \"\"\"Harry Potter is the most miserable, lonely boy you can imagine. Heâ€™s shunned by his relatives, the Dursleyâ€™s, that have raised him since he was an infant. Heâ€™s forced to live in the cupboard under the stairs, forced to wear his cousin Dudleyâ€™s hand-me-down clothes, and forced to go to his neighbourâ€™s house when the rest of the family is doing something fun. Yes, heâ€™s just about as miserable as you can get.\"\"\"\n",
    "    lower_text = to_lower(text)\n",
    "    print (lower_text)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Tokenize\n",
    "Tokenize words to get the tokens of the text i.e breaking the sentences into list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Harry', 'Potter', 'is', 'the', 'most', 'miserable', ',', 'lonely', 'boy', 'you', 'can', 'imagine', '.', 'He', \"'s\", 'shunned', 'by', 'his', 'relatives', ',', 'the', 'Dursley', \"'s\", ',', 'that', 'have', 'raised', 'him', 'since', 'he', 'was', 'an', 'infant', '.', 'He', \"'s\", 'forced', 'to', 'live', 'in', 'the', 'cupboard', 'under', 'the', 'stairs', ',', 'forced', 'to', 'wear', 'his', 'cousin', 'Dudley', \"'s\", 'hand-me-down', 'clothes', ',', 'and', 'forced', 'to', 'go', 'to', 'his', 'neighbour', \"'s\", 'house', 'when', 'the', 'rest', 'of', 'the', 'family', 'is', 'doing', 'something', 'fun', '.', 'Yes', ',', 'he', \"'s\", 'just', 'about', 'as', 'miserable', 'as', 'you', 'can', 'get', '.']\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def word_tokenize(text):\n",
    "    \"\"\"\n",
    "    take string input and return list of words.\n",
    "    use nltk.word_tokenize() to split the words.\n",
    "    \"\"\"\n",
    "    word_list=[]\n",
    "    for sentences in nltk.sent_tokenize(text):\n",
    "        for words in nltk.word_tokenize(sentences):\n",
    "            word_list.append(words)\n",
    "    return word_list\n",
    "    \n",
    "def main():\n",
    "    text = \"\"\"Harry Potter is the most miserable, lonely boy you can imagine. He's shunned by his relatives, the Dursley's, that have raised him since he was an infant. He's forced to live in the cupboard under the stairs, forced to wear his cousin Dudley's hand-me-down clothes, and forced to go to his neighbour's house when the rest of the family is doing something fun. Yes, he's just about as miserable as you can get.\"\"\"\n",
    "    print (word_tokenize(text))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===counting the word frequency===\n",
      "sabrina 4\n",
      "make 2\n",
      "chill 2\n",
      "academi 2\n",
      "could 2\n",
      "adventur 2\n",
      "stream 1\n",
      "satan 1\n",
      "crossov 1\n",
      "wherea 1\n",
      "onc 1\n",
      "unfortun 1\n",
      "art 1\n",
      "seri 1\n",
      "difficult 1\n",
      "time 1\n",
      "event 1\n",
      "new 1\n",
      "kiernan 1\n",
      "netflix 1\n",
      "school 1\n",
      "slight 1\n",
      "umbrella 1\n",
      "one 1\n",
      "similar 1\n",
      "cannibalis 1\n",
      "wait 1\n",
      "step 1\n",
      "perhap 1\n",
      "adjust 1\n",
      "like 1\n",
      "offbeat 1\n",
      "futur 1\n",
      "behind 1\n",
      "potter 1\n",
      "mind 1\n",
      "friend 1\n",
      "beyond 1\n",
      "even 1\n",
      "teas 1\n",
      "realis 1\n",
      "harri 1\n",
      "surpris 1\n",
      "high 1\n",
      "put 1\n",
      "first 1\n",
      "point 1\n",
      "u 1\n",
      "unseen 1\n",
      "movi 1\n",
      "seem 1\n",
      "shown 1\n",
      "minor 1\n",
      "ya 1\n",
      "shapka 1\n",
      "past 1\n",
      "materi 1\n",
      "riverdal 1\n",
      "refer 1\n",
      "ruggl 1\n",
      "servic 1\n"
     ]
    }
   ],
   "source": [
    "def pre_process(text):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    text = expand_contractions(text, contractions_dict)\n",
    "    text = autospell(text)\n",
    "    text = to_lower(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = remove_punct(text)\n",
    "    text = remove_Tags(text)\n",
    "    text = lemmatize(text)\n",
    "    text = stemming(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = word_tokenize(text)\n",
    "    return (text)\n",
    "\n",
    "\n",
    "def main():\n",
    "    text = \"\"\"Similaly, once you'd beyond this surprising first step, you'd realise that Satanism isn't at all like what the movies have shown us. Whereas Harry Potter couldn't wait to put his muggle past behind him, Sabrina is having a more difficult time making up her mind. At one point, Sabrina and her friends even make a slight reference to Riverdale high school, perhaps teasing a future crossover. Kiernan Shipka is off to the Academy of Unseen Arts in Chilling Adventures of Sabrina. With minor adjustments The Chilling Adventures of Sabrina could be Netflix's new A Series of Unfortunate Events, but with Umbrella Academy in the offing, the streaming service's offbeat YA material seems to be cannibalising itself.\"\"\"\n",
    "    pre_process_text = pre_process(text)\n",
    "    print (\"===counting the word frequency===\")\n",
    "    word_count = dict()\n",
    "\n",
    "    for word in pre_process_text:\n",
    "        if word in word_count:\n",
    "            word_count[word] += 1\n",
    "        else:\n",
    "            word_count[word] = 1\n",
    "\n",
    "    for w in sorted(word_count, key=word_count.get, reverse=True):\n",
    "        print (w, word_count[w])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
